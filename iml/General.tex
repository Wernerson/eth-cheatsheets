\section*{General}

$$
\operatorname{var}(AX)=A\operatorname{var}(X)A^\top
$$

$$
\begin{bsmallmatrix}
a&b \\ 
c&d
\end{bsmallmatrix}^{-1}=\frac{1}{ad-bc}
\begin{bsmallmatrix}
d&-b \\ 
-c&a
\end{bsmallmatrix}
$$

$$
\widehat{\operatorname{cov}}(X)=\frac{1}{n}X^\top X=\frac{1}{n}\sum_i x_ix_i^\top
$$

\subsection*{Convexity}
\begin{itemize}
    \item{$f(t x + (1-t)y) \leq t f(x) + (1-t) f(y)$}
    \item{$f$ convex, $g$ affine $\imp f\circ g$ convex}
    \item{$f$ convex, non-decreasing, $g$ convex $\imp f\circ g$ convex}
    \item{$f$ convex, $g$ affine $\imp f\circ g$ convex}
    \item{$\alpha, \beta \geq 0, f, g$ convex $\implies \alpha f + \beta g$ convex}
    \item pointwise maximum of two convex functions is convex
\end{itemize}

\subsection*{Gaussian}

For $\mu \in \R^p, \Sigma \in \R^{p \times p} \ \text{spd}$, $X$ has normal distribution with density:

$\frac{1}{\sqrt{(2\pi)^p \det(\Sigma))}}\exp({-\frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu)})$

\subsection*{PSD}
$M \in \mathbb{R}^{n\times n}$ PSD $\Leftrightarrow \forall x \in \mathbb{R}^n: x^\top Mx \geq 0 \\
\Leftrightarrow$ all principal minors of $M$ have non-negative determinant\\
$\Leftrightarrow \lambda \geq 0 \ \forall \lambda\in\sigma(M)$

\subsection*{Expected estimation and generalization error}
$\E_{X}\ell(f(X), f^\ast(X))$ \\ 
$\E_{X,Y}\ell(f(X), Y)$

\subsection*{Central Limit Theorem}

FÃ¼r $X_i$ iid mit $m = \E[X_1]$ und $\Var[X_1] = \sigma^2$ gilt $\P\left[\frac{\sum_{i=1}^n X_i - n m}{\sqrt{\sigma^2 n}} \leq a\right] \xrightarrow[n \to \infty]{} \Phi(a)$.

\subsection*{Gradient Descent}
$w_{t+1} = w_t - \eta \nabla L(w_t)$
\subsection*{Gradient Descent with Momentum} $w_{t+1}=w_t+m(w_t-w_{t-1})-\eta \nabla L(w_t)$
\subsection*{SGD}
Pick data poi nt $(x,y)$ u.a.r. and set\\  $w_{t+1} = w_t - \eta \nabla L_{(x,y)}(w_t)$ \\ For mini-batch SGD, choose $S \subseteq \{1, \ldots, n\}$ and set $w_{t+1} = w_t - \eta \nabla L_{S}(w_t)$
\subsection*{Complexity}
Matmul $A\in\R^{n\times k},B\in R^{k\times d}$: $\Theta(n k d)$

\subsection*{KL Divergence}
$D_{KL}(P||Q) = \mathbb{E}_p[\log(\frac{p(x)}{q(x)})]$, 0 iff $P = Q$, always non-negative
\\

\subsection*{Pseudoinverse}

For $A = U \Sigma V^\top \in \R^{m \times n}$ and $\Sigma^\dagger \in \R^{n \times m}$ inverse of non-zero singular values:

$$
A^\dagger = V \Sigma^\dagger U^\top
$$
