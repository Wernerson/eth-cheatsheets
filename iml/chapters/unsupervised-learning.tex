\section*{Unsupervised Learning}

\subsection*{k-Means Clustering}

Optimization Goal (non-convex):

\qquad $\hat{R} (\mu) = \sum_{i=1}^n \min_{j\in \{1,...,k\}} ||x_i - \mu_j||_2^2$

Lloyd's heuristics:
Init. cluster centers $\mu^{(0)}$:
\begin{compactitem}
	\item Assign points to closest center				
	\item Update $\mu_i$ as mean of assigned points
\end{compactitem}

Converges in exponential time.

Initialize with \textbf{k-Means++}:

\begin{compactitem}
	\item Random data point $\mu_1 = x_i$
	\item Add $\mu_2,...,\mu_k$ rand., with prob:
		$$\text{given } \mu_{1:j} \text{ pick } \mu_{j+1} = x_i$$ 
		$\text{ where } p(i) = \frac{1}{z} \min_{l \in \{1,...,j\}} ||x_i - \mu_l||_2^2$
\end{compactitem}
Converges in expectation $\mathcal O (\log k) * \text{opt. solution}$.
Find $k$ by negligible loss decrease or reg.

\subsection*{Principal Component Analysis}

Given centered data, the PCA problem is 
$$\min_{W^\top W=I_k,z_i\in\R^k}\sum_{i=1}^n||W z_i - x_i||_2^2,$$
with solution $W^* = (v_1|...|v_k)$ where $v_i$ are the ordered 
eigenvectors of $\frac{1}{n}\sum_ix_ix_i^\top$ 
and $z_i = {W^*}^\top x_i$. 

\subsection*{PCA through SVD}
\color{White} . \color {Black}\\[-10pt]
The first $k$ columns of $V$ where $X = U S V^\top$.

\subsection*{Kernel PCA}

The Kernel Principal Components are given by $\alpha^{1},...,\alpha^{k}\in \mathbb{R}^n$ 
where $\alpha^{i} = \frac{1}{\sqrt{\lambda_i}}v_i$ and 
$K = \sum_{i=1}^n \lambda_i v_i v_i^\top$ with ordered $\lambda_i.$ A point 
$x$ is projected to $z \in \mathbb{R}^k$:
$z_i = \sum_{j=1}^n\alpha_j^{(i)}k(x,x_j)$

\subsection*{Autoencoders}

We want to minimize $\frac{1}{n}\sum_{i=1}^n ||x_i - \hat{x}_i||_2^2$.
$$\hat{x} = f_{dec}(f_{enc}(x, \theta_{enc}); \theta_{dec})$$

Lin. activation func. \& square loss $=>$ PCA